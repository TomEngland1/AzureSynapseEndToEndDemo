{
	"name": "scala_Import csv files to lake",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "test",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "68b71f72-8d25-4a91-9315-215bb9204ed6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_spark",
				"display_name": "scala"
			},
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/8c446569-e418-4625-9c82-5fe80d7a0993/resourceGroups/rg-synapse-demo-uks/providers/Microsoft.Synapse/workspaces/ws-synapse-demo-uks/bigDataPools/test",
				"name": "test",
				"type": "Spark",
				"endpoint": "https://ws-synapse-demo-uks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/test",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"val container_name = \"dlsscdpocfs1\"\r\n",
					"val storage_account = \"dlsscdpoc\"\r\n",
					"val lake_path = \"raw/hes_apc/2024/01/22/17/19\"\r\n",
					"val sql_pool = \"scdpocws1p1\"\r\n",
					"val staging_table = \"spark_rawhes_apc202401221719\""
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"//Add required imports\r\n",
					"import org.apache.spark.sql.DataFrame\r\n",
					"import org.apache.spark.sql.SaveMode\r\n",
					"import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
					"import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
					"\r\n",
					"//Define read options for example, if reading from CSV source, configure header and delimiter options.\r\n",
					"val pathToInputSource= f\"abfss://$container_name@$storage_account.dfs.core.windows.net/$lake_path/*.csv\"\r\n",
					"\r\n",
					"//Define read configuration for the input CSV\r\n",
					"val dfReadOptions:Map[String, String] = Map(\"header\" -> \"true\", \"delimiter\" -> \",\")\r\n",
					"\r\n",
					"//Initialize DataFrame that reads CSV data from a given source \r\n",
					"val readDF:DataFrame=spark.\r\n",
					"            read.\r\n",
					"            options(dfReadOptions).\r\n",
					"            csv(pathToInputSource)//.\r\n",
					"            //limit(1000) //Reads first 1000 rows from the source CSV input.\r\n",
					"\r\n",
					"//Setup and trigger the read DataFrame for write to Synapse Dedicated SQL Pool.\r\n",
					"//Fully qualified SQL Server DNS name can be obtained using one of the following methods:\r\n",
					"//    1. Synapse Workspace - Manage Pane - SQL Pools - <Properties view of the corresponding Dedicated SQL Pool>\r\n",
					"//    2. From Azure Portal, follow the bread-crumbs for <Portal_Home> -> <Resource_Group> -> <Dedicated SQL Pool> and then go to Connection Strings/JDBC tab. \r\n",
					"//If `Constants.SERVER` is not provided, the value will be inferred by using the `database_name` in the three-part table name argument to the `synapsesql` method.\r\n",
					"//Like-wise, if `Constants.TEMP_FOLDER` is not provided, the connector will use the runtime staging directory config (see section on Configuration Options for details).\r\n",
					"val writeOptionsWithAADAuth:Map[String, String] = Map(Constants.SERVER -> \"scdpocws1.sql.azuresynapse.net\",\r\n",
					"                                            Constants.TEMP_FOLDER -> \"abfss://dlsscdpocfs1@dlsscdpoc.dfs.core.windows.net/spark_temp\")\r\n",
					"\r\n",
					"//Setup optional callback/feedback function that can receive post write metrics of the job performed.\r\n",
					"var errorDuringWrite:Option[Throwable] = None\r\n",
					"val callBackFunctionToReceivePostWriteMetrics: (Map[String, Any], Option[Throwable]) => Unit =\r\n",
					"    (feedback: Map[String, Any], errorState: Option[Throwable]) => {\r\n",
					"    println(s\"Feedback map - ${feedback.map{case(key, value) => s\"$key -> $value\"}.mkString(\"{\",\",\\n\",\"}\")}\")\r\n",
					"    errorDuringWrite = errorState\r\n",
					"}\r\n",
					"\r\n",
					"//Configure and submit the request to write to Synapse Dedicated SQL Pool (note - default SaveMode is set to ErrorIfExists)\r\n",
					"//Sample below is using AAD-based authentication approach; See further examples to leverage SQL Basic auth.\r\n",
					"readDF.\r\n",
					"    write.\r\n",
					"    //Configure required configurations.\r\n",
					"    options(writeOptionsWithAADAuth).\r\n",
					"    //Choose a save mode that is apt for your use case.\r\n",
					"    mode(SaveMode.Overwrite).\r\n",
					"    synapsesql(tableName = f\"scdpocws1p1.dbo.spark_$staging_table\", \r\n",
					"                //For external table type value is Constants.EXTERNAL\r\n",
					"                tableType = Constants.INTERNAL\r\n",
					"                //Optional parameter that is used to specify external table's base folder; defaults to `database_name/schema_name/table_name`\r\n",
					"                //location = None, \r\n",
					"                //Optional parameter to receive a callback.\r\n",
					"                //callBackHandle = Some(callBackFunctionToReceivePostWriteMetrics)\r\n",
					"                )\r\n",
					"\r\n",
					"//If write request has failed, raise an error and fail the Cell's execution.\r\n",
					"if(errorDuringWrite.isDefined) throw errorDuringWrite.get\r\n",
					""
				],
				"execution_count": 59
			}
		]
	}
}